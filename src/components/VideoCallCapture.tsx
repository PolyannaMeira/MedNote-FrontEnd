import { useEffect, useRef, useState, useImperativeHandle, forwardRef } from "react";

type Props = {
  language: "pt" | "en";
  onTranscriptChange?: (transcript: string) => void;
  patientName: string;
};

export type VideoCallCaptureHandle = {
  reset: () => void;
};

type Status = "idle" | "selecting" | "recording" | "done";

// Fun√ß√£o auxiliar para obter prefixo do falante
const getSpeakerPrefix = (
	speaker: "doctor" | "patient",
	patientName: string,
	language: "pt" | "en",
): string => {
	switch (speaker) {
		case "doctor":
			return language === "pt" ? "M√©dico: " : "Doctor: ";
		case "patient": {
			const name =
				patientName.trim() || (language === "pt" ? "Paciente" : "Patient");
			return `${name}: `;
		}
		default:
			return "Sistema: ";
	}
};

const VideoCallCapture = forwardRef<VideoCallCaptureHandle, Props>(
  ({ language, onTranscriptChange, patientName }, ref) => {
  const [status, setStatus] = useState<Status>("idle");
  const [error, setError] = useState<string | null>(null);
  const [durationSec, setDurationSec] = useState(0);
  const [audioDetected, setAudioDetected] = useState<boolean>(false);

  // Sistema de altern√¢ncia autom√°tica para videochamadas
  const speechCountRef = useRef(0);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  
  // Web Speech API refs para transcri√ß√£o real-time
  const recognitionRef = useRef<any>(null);
  const finalTranscriptRef = useRef<string>("");
  const lastTranscriptRef = useRef<string>("");

  const streamRef = useRef<MediaStream | null>(null);
  const timerRef = useRef<number | null>(null);

  const labels = {
    pt: {
      title: "√Åudio da Videochamada (Meet / Zoom / etc.)",
      start: "Capturar √°udio da aba",
      stop: "Parar captura", 
      duration: "Dura√ß√£o:",
      errorPermission: "‚ùå Permiss√£o negada. INSTRU√á√ïES:\n\n1Ô∏è‚É£ Clique em 'Capturar √°udio da aba'\n2Ô∏è‚É£ Selecione a ABA do Google Meet (n√£o 'Tela inteira')\n3Ô∏è‚É£ MARQUE a caixa 'Compartilhar √°udio da aba'\n4Ô∏è‚É£ Clique em 'Compartilhar'\n\nüí° Se n√£o funcionar, recarregue a p√°gina e tente novamente.",
      errorGeneral: "‚ùå Erro na captura. Verifique se o Meet est√° tocando √°udio e tente novamente.",
      statusRecording: "üî¥ Capturando √°udio da aba...",
      statusDone: "‚úÖ Transcri√ß√£o conclu√≠da.",
      troubleshooting: "üí° IMPORTANTE: Selecione a ABA do Meet e marque 'Compartilhar √°udio da aba'. Se n√£o funcionar, recarregue a p√°gina.",
      speechRecognitionError: "‚ùå Erro na transcri√ß√£o. Recarregue a p√°gina e tente novamente.",
      speechRecognitionUnsupported: "‚ùå Transcri√ß√£o n√£o suportada neste navegador. Use Chrome ou Edge."
    },
    en: {
      title: "Video Call Audio (Meet / Zoom / etc.)",
      start: "Capture tab audio",
      stop: "Stop capture",
      duration: "Duration:",
      errorPermission: "‚ùå Permission denied. INSTRUCTIONS:\n\n1Ô∏è‚É£ Click 'Capture tab audio'\n2Ô∏è‚É£ Select Google Meet TAB (not 'Entire screen')\n3Ô∏è‚É£ CHECK 'Share tab audio' checkbox\n4Ô∏è‚É£ Click 'Share'\n\nüí° If it doesn't work, reload the page and try again.",
      errorGeneral: "‚ùå Capture error. Check if Meet is playing audio and try again.",
      
      statusRecording: "üî¥ Capturing tab audio...",
      statusDone: "‚úÖ Transcription completed.",
      troubleshooting: "üí° IMPORTANT: Select Meet TAB and check 'Share tab audio'. If it fails, reload the page.",
      speechRecognitionError: "‚ùå Transcription error. Reload the page and try again.",
      speechRecognitionUnsupported: "‚ùå Transcription not supported in this browser. Use Chrome or Edge."
    }
  };

  const t = labels[language];

  // Setup de √°udio para detectar se h√° som chegando
  const setupAudioDetection = () => {
    if (!streamRef.current) return;
    
    try {
      const audioContext = new AudioContext();
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(streamRef.current);
      source.connect(analyser);
      
      analyser.fftSize = 256;
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      
      const checkAudio = () => {
        if (!analyserRef.current) return;
        
        analyserRef.current.getByteFrequencyData(dataArray);
        const sum = dataArray.reduce((acc, val) => acc + val, 0);
        const average = sum / dataArray.length;
        
        // Se detectar √°udio acima de threshold
        if (average > 10) {
          setAudioDetected(true);
        }
        
        if (status === "recording") {
          requestAnimationFrame(checkAudio);
        }
      };
      
      checkAudio();
    } catch (err) {
      console.warn("Erro ao configurar detec√ß√£o de √°udio:", err);
    }
  };

  const cleanup = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }

    if (timerRef.current) {
      window.clearInterval(timerRef.current);
      timerRef.current = null;
    }

    // Limpa contexto de √°udio
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
      analyserRef.current = null;
    }
    
    // Limpa speech recognition
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      recognitionRef.current = null;
    }
  };

  // Setup Web Speech API para transcri√ß√£o real-time
  useEffect(() => {
    if (!("webkitSpeechRecognition" in window)) {
      console.error("Speech recognition not supported");
      return;
    }

    const SpeechRecognition = (window as any).webkitSpeechRecognition;
    const rec = new SpeechRecognition();
    rec.continuous = true;
    rec.interimResults = true;
    rec.lang = language === "pt" ? "pt-BR" : "en-US";

    rec.onresult = async (event: any) => {
      let interimTranscript = "";
      let finalTranscript = "";

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcriptText = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcriptText + " ";
        } else {
          interimTranscript += transcriptText;
        }
      }

      if (finalTranscript) {
        // Evita processar a mesma fala duas vezes
        const trimmedText = finalTranscript.trim();
        if (trimmedText === lastTranscriptRef.current) {
          return;
        }
        lastTranscriptRef.current = trimmedText;
        
        // Determina falante baseado na altern√¢ncia autom√°tica para videochamadas
        const speakerType: "doctor" | "patient" = speechCountRef.current % 2 === 0 ? "doctor" : "patient";
        
        // Incrementa contador
        speechCountRef.current += 1;
        
        // Adiciona identifica√ß√£o do falante ao texto final
        const speakerPrefix = getSpeakerPrefix(speakerType, patientName, language);
        const formattedText = `${speakerPrefix}${trimmedText}\n`;
        finalTranscriptRef.current += formattedText;

        // Atualiza apenas o componente pai, sem estado local
        const fullTranscript = finalTranscriptRef.current + (interimTranscript ? interimTranscript : "");
        if (onTranscriptChange) {
          onTranscriptChange(fullTranscript);
        }
      } else if (interimTranscript) {
        // Apenas notifica o componente pai com texto interim
        const fullTranscript = finalTranscriptRef.current + interimTranscript;
        if (onTranscriptChange) {
          onTranscriptChange(fullTranscript);
        }
      }
    };

    rec.onerror = (event: any) => {
      console.error("Speech recognition error:", event.error);
    };

    recognitionRef.current = rec;

    return () => {
      if (rec) {
        rec.stop();
      }
    };
  }, [language, onTranscriptChange, patientName]);

  // Fun√ß√£o de reset para nova consulta
  const reset = () => {
    cleanup();
    setStatus("idle");
    setError(null);
    setDurationSec(0);
    setAudioDetected(false);
    speechCountRef.current = 0;
    finalTranscriptRef.current = "";
    lastTranscriptRef.current = "";
  };

  // Exp√µe a fun√ß√£o reset para o componente pai
  useImperativeHandle(ref, () => ({
    reset,
  }));

  useEffect(() => {
    return () => {
      cleanup();
    };
  }, []);

  const startCapture = async () => {
    setError(null);
    setDurationSec(0);
    setStatus("selecting");

    try {
      
      const stream = await (navigator.mediaDevices as any).getDisplayMedia({
        video: true,
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false
        }
      });

      
      // checar se tem track de √°udio
      const audioTracks = stream.getAudioTracks();
      const hasAudio = audioTracks.length > 0;
      
      if (!hasAudio) {
        
        stream.getTracks().forEach((t: MediaStreamTrack) => t.stop());
        setStatus("idle");
        setError(t.errorPermission);
        return;
      }

      
      audioTracks.forEach((track: MediaStreamTrack, index: number) => {
        console.log(`Track ${index}:`, {
          id: track.id,
          kind: track.kind,
          label: track.label,
          enabled: track.enabled,
          muted: track.muted,
          readyState: track.readyState,
          settings: track.getSettings()
        });
      });

      
      const enabledAudioTracks = audioTracks.filter((track: MediaStreamTrack) => track.enabled);
      if (enabledAudioTracks.length === 0) {
        
      }

      
      streamRef.current = stream;

      // Reset contadores e transcri√ß√µes
      finalTranscriptRef.current = "";
      lastTranscriptRef.current = "";
      speechCountRef.current = 0;
      
      // Verificar se Speech Recognition est√° dispon√≠vel
      if (!recognitionRef.current) {
        console.error('‚ùå Speech Recognition n√£o foi inicializado');
        stream.getTracks().forEach((t: MediaStreamTrack) => t.stop());
        setStatus("idle");
        setError(t.speechRecognitionUnsupported);
        return;
      }
      
      try {
        // Verificar se j√° est√° rodando e parar antes de reiniciar
        try {
          recognitionRef.current.stop();
          // Pequena pausa para garantir que parou
          await new Promise(resolve => setTimeout(resolve, 100));
        } catch (stopError) {
          // Ignorar erro se n√£o estava rodando
        }
        
        // Iniciar transcri√ß√£o
        recognitionRef.current.start();
        setStatus("recording");
        setupAudioDetection();
        
      } catch (recognitionError: any) {
        console.error('‚ùå Erro ao iniciar Speech Recognition:', recognitionError);
        stream.getTracks().forEach((t: MediaStreamTrack) => t.stop());
        setStatus("idle");
        
        if (recognitionError.error === 'already-started') {
          setError(language === "pt" ? 
            "‚ùå Transcri√ß√£o j√° est√° ativa. Recarregue a p√°gina e tente novamente." :
            "‚ùå Transcription already active. Reload the page and try again.");
        } else {
          setError(t.speechRecognitionError);
        }
        return;
      }

      // contador de tempo
      let sec = 0;
      timerRef.current = window.setInterval(() => {
        sec += 1;
        setDurationSec(sec);
      }, 1000);
    } catch (err: any) {
      console.error("‚ùå Erro ao capturar aba:", err);
      setStatus("idle");
      
      // Erro espec√≠fico baseado no tipo
      if (err.name === "NotAllowedError") {
        setError(t.errorPermission);
      } else if (err.name === "NotFoundError") {
        setError(t.errorGeneral);
      } else {
        setError(`${t.errorGeneral} (${err.message})`);
      }
    }
  };

  const stopCapture = () => {
    
    // Para o speech recognition
    if (recognitionRef.current) {
      recognitionRef.current.stop();
      
    }
    
    cleanup();
    setStatus("done");
    
    // Notifica com transcript final
    if (onTranscriptChange && finalTranscriptRef.current) {
      onTranscriptChange(finalTranscriptRef.current);
    }
  };

  const formatTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, "0")}`;
  };

  const statusText = () => {
    switch (status) {
      case "selecting":
        return "Selecionando aba...";
      case "recording":
        return t.statusRecording;
      case "done":
        return t.statusDone;
      default:
        return "";
    }
  };

  return (
    <div className="bg-white border border-gray-200 rounded-lg p-6 shadow-sm">
      <div className="space-y-4">
        <h3 className="text-lg font-semibold text-gray-800">{t.title}</h3>

        {/* Status e controles */}
        <div className="flex flex-wrap items-center gap-3">
          {status === "idle" && (
            <button
              onClick={startCapture}
              className="bg-blue-500 hover:bg-blue-600 text-white px-4 py-2 rounded-md font-medium transition-colors"
            >
              {t.start}
            </button>
          )}
          
          {status === "recording" && (
            <button
              onClick={stopCapture}
              className="bg-red-500 hover:bg-red-600 text-white px-4 py-2 rounded-md font-medium transition-colors"
            >
              üõë {t.stop}
            </button>
          )}
          
          <div className="text-sm text-gray-600 min-w-0">
            {status !== "idle" ? statusText() : ""}
          </div>
          
          {status === "recording" && (
            <>
              <div className="text-sm text-gray-600 whitespace-nowrap">
                {t.duration} {formatTime(durationSec)}
              </div>
              {audioDetected && (
                <div className="text-green-600 text-sm whitespace-nowrap">
                  üéµ {language === "pt" ? "√Åudio detectado" : "Audio detected"}
                </div>
              )}
            </>
          )}
        </div>

        {/* Erros com instru√ß√µes detalhadas */
        {error && (
          <div className="bg-red-50 border border-red-200 rounded-md p-4">
            <pre className="text-red-800 text-sm whitespace-pre-line font-sans">{error}</pre>
          </div>
        )}

        {/* Instru√ß√µes sempre vis√≠veis */}
        <div className="bg-blue-50 border border-blue-200 rounded-md p-4">
          <div className="text-blue-700 text-xs leading-relaxed">
            {t.troubleshooting}
          </div>
        </div>
      </div>
    </div>
  );
});

VideoCallCapture.displayName = "VideoCallCapture";

export default VideoCallCapture;